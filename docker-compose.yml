version: '3.8'

services:
  #### MINIO ####
  s3:
    build:
      context: .
      dockerfile: ./docker/dockerfile.minio
    container_name: "s3"
    ports:
      - "9001:9001"
      - "9000:9000"
    #volumes:
      #- ./docker/object_storage:/minio/storedData
    environment:
      MINIO_ROOT_USER: "${MINIO_USER}"
      MINIO_ROOT_PASSWORD: "${MINIO_PASSWORD}"

  # Airflow
  #  airflow:
  #    image: bitnami/airflow:2.2.5
  #    container_name: "airflow"
  #    ports:
  #      - "8080:8080"
  #    env_file:
  #      - openlineage.env
  #    environment:
  #      - AIRFLOW_USERNAME=${AIRFLOW_USER}
  #      - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
  #      - AIRFLOW_EMAIL=airfloow@example.com
  #      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
  #      - AIRFLOW_DATABASE_HOST=db
  #      - AIRFLOW_DATABASE_NAME=${AIRFLOW_DB}
  #      - AIRFLOW_DATABASE_USERNAME=${AIRFLOW_USER}
  #      - AIRFLOW_DATABASE_PASSWORD=${AIRFLOW_PASSWORD}
  #      - AIRFLOW_EXECUTOR=CeleryExecutor
  #      - AIRFLOW_LOAD_EXAMPLES=no
  #      - AIRFLOW_CONN_EXAMPLE_DB=postgres://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@db:5432/${AIRFLOW_DB}
  #      - BITNAMI_DEBUG=true
  #    volumes:
  #      - ./realestatescrape/dags:/opt/bitnami/airflow/dags
  #      - ${PWD}/realestatescrape/whl:/whl
  #      - type: bind
  #        source: ${PWD}/requirements.txt
  #        target: /bitnami/python/requirements.txt
  #
  #  airflow_scheduler:
  #    image: bitnami/airflow-scheduler:2.2.5
  #    container_name: "airflow-scheduler"
  #    env_file:
  #      - openlineage.env
  #    environment:
  #      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
  #      - AIRFLOW_DATABASE_HOST=db
  #      - AIRFLOW_DATABASE_NAME=${AIRFLOW_DB}
  #      - AIRFLOW_DATABASE_USERNAME=${AIRFLOW_USER}
  #      - AIRFLOW_DATABASE_PASSWORD=${AIRFLOW_PASSWORD}
  #      - AIRFLOW_WEBSERVER_HOST=airflow
  #      - AIRFLOW_EXECUTOR=CeleryExecutor
  #      - AIRFLOW_LOAD_EXAMPLES=no
  #      - AIRFLOW_CONN_EXAMPLE_DB=postgres://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@db:5432/${AIRFLOW_DB}
  #      - BITNAMI_DEBUG=true
  #    volumes:
  #      - ./realestatescrape/dags:/opt/bitnami/airflow/dags
  #      - ${PWD}/realestatescrape/whl:/whl
  #      - type: bind
  #        source: ${PWD}/requirements.txt
  #        target: /bitnami/python/requirements.txt
  #
  #  airflow_worker:
  #    image: bitnami/airflow-worker:2.2.5
  #    container_name: "airflow-worker"
  #    env_file:
  #      - openlineage.env
  #    environment:
  #      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
  #      - AIRFLOW_DATABASE_HOST=db
  #      - AIRFLOW_DATABASE_NAME=${AIRFLOW_DB}
  #      - AIRFLOW_DATABASE_USERNAME=${AIRFLOW_USER}
  #      - AIRFLOW_DATABASE_PASSWORD=${AIRFLOW_PASSWORD}
  #      - AIRFLOW_WEBSERVER_HOST=airflow
  #      - AIRFLOW_EXECUTOR=CeleryExecutor
  #      - AIRFLOW_LOAD_EXAMPLES=no
  #      - AIRFLOW_CONN_EXAMPLE_DB=postgres://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@db:5432/${AIRFLOW_DB}
  #      - BITNAMI_DEBUG=true
  #    volumes:
  #      - ./realestatescrape/dags:/opt/bitnami/airflow/dags
  #      - ${PWD}/realestatescrape/whl:/whl
  #      - type: bind
  #        source: ${PWD}/requirements.txt
  #        target: /bitnami/python/requirements.txt
  #
  #  redis:
  #    container_name: "airflow-redis"
  #    image: bitnami/redis:6.0.6
  #    environment:
  #      - ALLOW_EMPTY_PASSWORDS=yes
  #      - BITNAMI_DEBUG=true

  #### Marquez ####
  api:
    image: "marquezproject/marquez:${TAG}"
    container_name: marquez-api
    environment:
      - MARQUEZ_PORT=${API_PORT}
      - MARQUEZ_ADMIN_PORT=${API_ADMIN_PORT}
    volumes:
      - ./docker/wait-for-it.sh:/opt/marquez/wait-for-it.sh
      - ./docker/entrypoint.sh:/opt/marquez/entrypoint.sh
    links:
      - "db:postgres"
    depends_on:
      - db
    entrypoint: ["/opt/marquez/wait-for-it.sh", "db:5432", "--", "./entrypoint.sh"]
    # Enables SQL statement logging (see: https://www.postgresql.org/docs/12/runtime-config-logging.html#GUC-LOG-STATEMENT)
    # command: ["postgres", "-c", "log_statement=all"]

  web:
    image: "marquezproject/marquez-web:${TAG}"
    container_name: marquez-web
    environment:
      - MARQUEZ_HOST=api
      - MARQUEZ_PORT=${API_PORT}
    ports:
      - "${WEB_PORT}:${WEB_PORT}"
    stdin_open: true
    tty: true
    depends_on:
      - api
  db:
    image: bitnami/postgresql:12.1.0
    container_name: marquez-db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - MARQUEZ_DB=${MARQUEZ_DB}
      - MARQUEZ_USER=${MARQUEZ_USER}
      - MARQUEZ_PASSWORD=${MARQUEZ_PASSWORD}
      - AIRFLOW_DB=${AIRFLOW_DB}
      - AIRFLOW_USER=${AIRFLOW_USER}
      - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
      - EXAMPLE_USER=example
      - EXAMPLE_PASSWORD=example
      - EXAMPLE_DB=example
      - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - ./docker/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
    # Enables SQL statement logging (see: https://www.postgresql.org/docs/12/runtime-config-logging.html#GUC-LOG-STATEMENT)
    #command: ["postgres", "-c", "log_statement=all"]

  #### Spark ####
  spark-master:
    build:
      context: .
      dockerfile: ./docker/dockerfile.spark
    container_name: "spark-master"
    ports:
      - "9090:8080"
      - "7070:7070"
    volumes:
      - ./docker/apps:/opt/spark-apps
      - ./docker/data:/opt/spark-data
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
  spark-worker-a:
    build:
      context: .
      dockerfile: ./docker/dockerfile.spark
    container_name: "spark-worker-a"
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
      - ./docker/apps:/opt/spark-apps
      - ./docker/data:/opt/spark-data
  spark-worker-b:
    build:
      context: .
      dockerfile: ./docker/dockerfile.spark
    container_name: "spark-worker-b"
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    volumes:
      - ./docker/apps:/opt/spark-apps
      - ./docker/data:/opt/spark-data
